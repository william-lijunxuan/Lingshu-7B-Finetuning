{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhlHYZysncF-",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -U transformers trl[vllm] datasets tensorflow fastai gensim wandb\n",
    "# Tested with python 3.12 and !pip install transformers==4.57.3 trl[vllm]==4.4.1 torch==2.8.0+cu128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KGgPgk_5S8r"
   },
   "source": [
    "## Dataset processing\n",
    "\n",
    "This notebook uses the [MedQA](https://arxiv.org/abs/2009.13081) dataset, a multiple-choice question dataset derived from medical licensing exams in the US, China, and Taiwan, designed to assess medical knowledge and clinical reasoning skills.\n",
    "\n",
    "Load the data using the Hugging Face `datasets` library. Then, create train and validation splits. We subsample the dev split for faster evaluation times.\n",
    "\n",
    "**Dataset citation:** Jin, D., Pan, E., Oufattole, N., Weng, W. H., Fang, H., & Szolovits, P. (2021). What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14), 6421."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "\n",
    "def process_medqa(data):\n",
    "    prompt_template = f\"\"\"Answer the given question. Think step by step.\n",
    "    You can directly provide the answer (A single letter), without further additions. E.g. \"Final Answer: (A)\".\n",
    "    Question: [QUESTION]\n",
    "    [OPTIONS]\n",
    "    \"\"\"\n",
    "    return data.map(lambda x: {\n",
    "                        'prompt': [\n",
    "                            {'role': 'system', 'content': 'SYSTEM INSTRUCTION: think silently if needed.'},\n",
    "                            {'role': 'user', 'content': prompt_template.replace('[QUESTION]', x['data']['Question']).replace(\n",
    "                                '[OPTIONS]', f\"(A) {x['data']['Options']['A']} (B) {x['data']['Options']['B']} (C) {x['data']['Options']['C']} (D) {x['data']['Options']['D']}\")}\n",
    "                        ],\n",
    "                        'answer': x['data']['Correct Option']\n",
    "                    })\n",
    "\n",
    "medqa_dataset = datasets.load_dataset(\"openlifescienceai/medqa\")\n",
    "train_dataset = process_medqa(medqa_dataset[\"train\"])\n",
    "val_dataset = process_medqa(medqa_dataset[\"dev\"])"
   ],
   "metadata": {
    "id": "gblttaNEwmo0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset['prompt'][0]"
   ],
   "metadata": {
    "id": "HdGgjvd06QrR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f42b00aa-3dfc-46f3-a791-a2aac944e0f8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'content': 'SYSTEM INSTRUCTION: think silently if needed.',\n",
       "  'role': 'system'},\n",
       " {'content': 'Answer the given question. Think step by step.\\n  You can directly provide the answer (A single letter), without further additions. E.g. \"Final Answer: (A)\".\\n  Question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\\n  (A) Ampicillin (B) Ceftriaxone (C) Doxycycline (D) Nitrofurantoin\\n  ',\n",
       "  'role': 'user'}]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Post-train the model with LoRA via GRPO on MedQA\n",
    "\n",
    "Traditional fine-tuning of large language models is resource-intensive because it requires adjusting billions of parameters. Parameter-Efficient Fine-Tuning (PEFT) addresses this by training a smaller number of parameters. A common PEFT technique is *Low-Rank Adaptation (LoRA)*, which efficiently adapts large language models by training small, low-rank matrices that are added to the original model instead of updating the full-weight matrices.\n",
    "\n",
    "*GRPO (Group Relative Policy Optimization)* is a reinforcement learning (RL) algorithm that aims to improve efficiency and reduce training costs by eliminating the need for a separate value function. Instead, GRPO uses group-based advantage estimation and incorporates KL divergence into the loss function for better stability.\n",
    "\n",
    "This notebook demonstrates RL training MedGemma (with verifiable rewards) with LoRA."
   ],
   "metadata": {
    "id": "SwzOobP8OTGK"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KihAIYFJjTeR"
   },
   "source": [
    "First, define the reward function to check when the model's answer letter matches the correct answer letter (i.e. 'A', 'B', 'C', or 'D')."
   ]
  },
  {
   "metadata": {
    "id": "4JFidSR3Z0SW"
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def extract_xml_answer(answer: str) -> str:\n",
    "    \"\"\"Extract the answer letter from an XML answer string.\"\"\"\n",
    "    if not isinstance(answer, str):\n",
    "        return None\n",
    "    if not answer:\n",
    "        return None\n",
    "\n",
    "    final_answers = [\n",
    "        r'The final answer is\\s\\(([A-J])\\)',\n",
    "        r'The final answer is\\s\\**\\(([A-J])\\)\\**',\n",
    "        r'The final answer is\\s\\$\\\\boxed{([A-J])}\\$',\n",
    "        r'Final Answer:\\(([A-J])\\)',\n",
    "        r'Final Answer:\\s\\(([A-J])\\)',\n",
    "        r'Final Answer:\\s\\(?([A-J])',\n",
    "        r'Final Answer:\\s*\\**\\(([A-J])\\)\\**',\n",
    "        r'\\**Final Answer:\\**\\s\\(([A-J])\\)',\n",
    "    ]\n",
    "    for final_type in final_answers:\n",
    "        match = re.search(final_type, answer)\n",
    "        if match:\n",
    "            answer_letter = match.group(1)\n",
    "            return f'{answer_letter}'\n",
    "\n",
    "    return None\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function to check when the model's answer letter matches the correct answer letter (i.e. 'A', 'B', ...)\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    # print(f\"-----Question:\\n{q}\\nAnswer:\\n{answer[0]}\\nResponse:\\n{responses[0]}\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    # print([(r,a, r == a) for r, a in zip(extracted_responses, answer)])\n",
    "    return [1.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux6iqP7z5YOo"
   },
   "source": [
    "Next, configure training with the `GRPOConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzOuSVCL_GA9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "ckpt = \"google/medgemma-1.5-4b-it\"\n",
    "output_dir=\"./tuned_medgemma4b\",\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    eval_on_start=False,                     # Run an evaluation at the very beginning of training.\n",
    "    learning_rate=5e-6,                      # The initial learning rate for the AdamW optimizer.\n",
    "    per_device_train_batch_size=3,\n",
    "    gradient_accumulation_steps=4,           # Accumulate gradients for this many steps to simulate a larger batch size (per_device_train_batch_size * gradient_accumulation_steps).\n",
    "    num_generations=4,                       # Number of completions to generate per prompt for GRPO's preference learning.\n",
    "    max_prompt_length=512,                   # Maximum token length for input prompts.\n",
    "    max_completion_length=1024,              # Maximum token length for the model's generated completions.\n",
    "    max_steps=1700,\n",
    "    logging_steps=20,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "    use_vllm=True,                           # Use the vLLM library for significantly faster inference during generation.\n",
    "    vllm_mode=\"colocate\",                    # vLLM deployment mode; 'colocate' runs vLLM on the same GPU(s) as the trainer.\n",
    "    vllm_gpu_memory_utilization=.30,         # Fraction of GPU memory that vLLM is allowed to use.\n",
    "    bf16=True,                               # Enable bfloat16 mixed precision training to save memory and speed up training.\n",
    "    gradient_checkpointing=True,             # Save memory by trading compute (avoids storing all intermediate activations).\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False               # Use a more efficient implementation of gradient checkpointing.\n",
    "    },\n",
    "    model_init_kwargs={\n",
    "        \"device_map\": \"auto\",\n",
    "        \"dtype\": torch.bfloat16,             # Set model parameter data type to bfloat16.\n",
    "        \"attn_implementation\": \"eager\"       # Gemma 3 recommends using the 'eager' attention implementation.\n",
    "    },\n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    target_modules=\"all-linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the model.\n",
    "\n",
    "Note that this will take a long time to run (~11 hrs total on A100 40GB GPU)."
   ],
   "metadata": {
    "id": "jKx8iw7n4DP_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=ckpt,\n",
    "    reward_funcs=[correctness_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset.select(range(100)), # Use a very small subset for validation\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir=training_args.output_dir)"
   ],
   "metadata": {
    "id": "5Us06pUdoAkX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Change the relevant paths to store training results in Google Drive\n",
    "if \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\"):\n",
    "    ! cp -r ./tuned_medgemma4b/ /content/drive/MyDrive/trl_colab_storage/"
   ],
   "metadata": {
    "id": "EKyeDdVQGWZ7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize training curves\n",
    "! pip install -q tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/tuned_medgemma4b/ --port 6007"
   ],
   "metadata": {
    "id": "eRM7_RBrB2PT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model evaluation: Effect of RL-tuning\n",
    "\n",
    "**Important: Before you continue, you may need to restart the runtime due to the VRAM limitation on Colab kernels.**\n",
    "\n",
    "The following cells compute and print the accuracy of the baseline and fine-tuned models on the test dataset to assess the effect of RL-tuning.\n",
    "\n",
    "We also load and process the test split using the same logic as before. These functions are repeated below for convenience."
   ],
   "metadata": {
    "id": "7dobbKZuhe_T"
   }
  },
  {
   "metadata": {
    "id": "StAJw0Z00LNN"
   },
   "cell_type": "code",
   "source": [
    "# Reinstantiate environment variables\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\"):\n",
    "    # Use secret if running in Google Colab\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "else:\n",
    "    # Store Hugging Face data under `/content` if running in Colab Enterprise\n",
    "    if os.environ.get(\"VERTEX_PRODUCT\") == \"COLAB_ENTERPRISE\":\n",
    "        os.environ[\"HF_HOME\"] = \"/content/hf\"\n",
    "    # Authenticate with Hugging Face\n",
    "    from huggingface_hub import get_token\n",
    "    if get_token() is None:\n",
    "        from huggingface_hub import notebook_login\n",
    "        notebook_login()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "yVBrWg5MjOZ_"
   },
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "import re\n",
    "\n",
    "def extract_xml_answer(answer: str):\n",
    "    \"\"\"Extract the answer letter from an XML answer string.\"\"\"\n",
    "    if not isinstance(answer, str):\n",
    "        return None\n",
    "    if not answer:\n",
    "        return None\n",
    "\n",
    "    final_answers = [\n",
    "        r'The final answer is\\s\\(([A-J])\\)',\n",
    "        r'The final answer is\\s\\**\\(([A-J])\\)\\**',\n",
    "        r'The final answer is\\s\\$\\\\boxed{([A-J])}\\$',\n",
    "        r'Final Answer:\\(([A-J])\\)',\n",
    "        r'Final Answer:\\s\\(([A-J])\\)',\n",
    "        r'Final Answer:\\s\\(?([A-J])',\n",
    "        r'Final Answer:\\s*\\**\\(([A-J])\\)\\**',\n",
    "        r'\\**Final Answer:\\**\\s\\(([A-J])\\)',\n",
    "    ]\n",
    "    for final_type in final_answers:\n",
    "        match = re.search(final_type, answer)\n",
    "        if match:\n",
    "            answer_letter = match.group(1)\n",
    "            return f'{answer_letter}'\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_medqa(data):\n",
    "    prompt_template = f\"\"\"Answer the given question. Think step by step.\n",
    "    You can directly provide the answer (A single letter), without further additions. E.g. \"Final Answer: (A)\".\n",
    "    Question: [QUESTION]\n",
    "    [OPTIONS]\n",
    "    \"\"\"\n",
    "    return data.map(lambda x: {\n",
    "                        'prompt': [\n",
    "                            {'role': 'system', 'content': 'SYSTEM INSTRUCTION: think silently if needed.'},\n",
    "                            {'role': 'user', 'content': prompt_template.replace('[QUESTION]', x['data']['Question']).replace(\n",
    "                                '[OPTIONS]', f\"(A) {x['data']['Options']['A']} (B) {x['data']['Options']['B']} (C) {x['data']['Options']['C']} (D) {x['data']['Options']['D']}\")}\n",
    "                        ],\n",
    "                        'answer': x['data']['Correct Option']\n",
    "                    })\n",
    "\n",
    "medqa_dataset = datasets.load_dataset(\"openlifescienceai/medqa\")\n",
    "test_dataset = process_medqa(medqa_dataset[\"test\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a method to run batch inference on the test dataset."
   ],
   "metadata": {
    "id": "yK3pEeMIX8El"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_inference_batched(test_dataset, model, processor, batch_size=4, device=\"cuda\", verbose=True):\n",
    "    \"\"\"\n",
    "    Runs inference on a processed test dataset using batching for efficiency.\n",
    "\n",
    "    Args:\n",
    "        test_dataset: A dataset where each item has 'prompt' (chat history) and 'answer' (ground truth).\n",
    "        model: The loaded PEFT model for inference.\n",
    "        processor: The processor for tokenizing the input.\n",
    "        batch_size (int): The number of samples to process at once. Adjust based on VRAM.\n",
    "        device (str): The device to run inference on ('cuda' or 'cpu').\n",
    "        verbose (bool): Whether to print progress and sample outputs.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, with each dictionary containing the prompt,\n",
    "        ground truth answer, and the model's generated answer.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Create an iterator for the batches\n",
    "    num_samples = len(test_dataset)\n",
    "\n",
    "    # Use tqdm for a progress bar if verbose is True\n",
    "    batch_iterator = range(0, num_samples, batch_size)\n",
    "    if verbose:\n",
    "        print(f\"Starting batched inference on {num_samples} samples with batch size {batch_size}...\")\n",
    "        batch_iterator = tqdm(batch_iterator, desc=\"Batch Inference\")\n",
    "\n",
    "    for i in batch_iterator:\n",
    "        # 1. Prepare the current batch\n",
    "        batch_data = test_dataset[i : i + batch_size]\n",
    "        batch_prompts = batch_data['prompt']\n",
    "        batch_ground_truths = batch_data['answer']\n",
    "\n",
    "        # 2. Tokenize the entire batch at once with left-padding\n",
    "        inputs = processor.tokenizer.apply_chat_template(\n",
    "            batch_prompts,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,  # Pad sequences to the length of the longest in the batch\n",
    "            max_length=1024,  # Set a fixed maximum length\n",
    "        ).to(device)\n",
    "\n",
    "        # 3. Generate responses for the entire batch in one go\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False, # Greedy decoding for deterministic output appropriate for logical reasoning\"\n",
    "        )\n",
    "\n",
    "        # 4. Decode the generated part of the output\n",
    "        # This is more robust than decoding the whole sequence and stripping the prompt\n",
    "        input_token_length = inputs['input_ids'].shape[1]\n",
    "        generated_tokens = outputs[:, input_token_length:]\n",
    "        model_generated_answers = processor.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # 5. Store the results for the current batch\n",
    "        for j in range(len(batch_prompts)):\n",
    "            results.append({\n",
    "                'prompt': batch_prompts[j],\n",
    "                'ground_truth': batch_ground_truths[j],\n",
    "                'model_answer': model_generated_answers[j].strip() # Use .strip() for clean output\n",
    "            })\n",
    "\n",
    "    # Optional: print a few examples from the final results\n",
    "    if verbose:\n",
    "        print(\"\\n--- Sample of Batched Inference Results ---\")\n",
    "        for res in results[:3]: # Print first 3 results\n",
    "            print(f\"Ground Truth: {res['ground_truth']}\")\n",
    "            print(f\"Model Answer: {res['model_answer']}\\n\")\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "id": "8QqVAuZolzBr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "FtBOKjuYlGqs"
   },
   "cell_type": "markdown",
   "source": [
    "### Evaluate baseline performance\n",
    "\n",
    "This cell calculates the baseline model's accuracy on the test data. This baseline serves as a benchmark to measure the fine-tuned model's performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Gemma3Processor, AutoModelForCausalLM\n",
    "\n",
    "# Load model and processor\n",
    "print(\"Loading model and processor...\")\n",
    "ckpt = \"google/medgemma-1.5-4b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = Gemma3Processor.from_pretrained(ckpt)\n",
    "\n",
    "# Run inference on the test dataset\n",
    "inference_results = run_inference_batched(\n",
    "    test_dataset=test_dataset,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    batch_size=32,\n",
    ")\n",
    "results_df = pd.DataFrame(inference_results)\n",
    "results_df['model_pred'] = results_df['model_answer'].apply(extract_xml_answer)\n",
    "results_df['correct'] = results_df['ground_truth'] == results_df['model_pred']\n",
    "print('Baseline Accuracy', results_df['correct'].mean())\n",
    "results_df.to_csv('baseline_test_results.csv') # Save baseline results\n",
    "\n",
    "del model # To free up VRAM\n",
    "del processor\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "TRDvIeBUvHOs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "845fbb22-e081-485b-bc55-f1a2acf66865"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Baseline Accuracy 0.14072327044025157\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected, we observe a low accuracy of 14% with only 1k output tokens. Let us look at an example output."
   ],
   "metadata": {
    "id": "pH2jMNF9ZRtn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(results_df['model_answer'].values[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYd53N0VZFTO",
    "outputId": "ef1d7f8e-789b-47d3-9aa8-88675842626f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<unused94>thought\n",
      "The user wants me to identify the antibiotic that blocks cell wall synthesis based on the clinical presentation and lab findings.\n",
      "\n",
      "1.  **Analyze the clinical presentation:** A young, sexually active male presents with symptoms suggestive of urethritis (fever, dysuria) and septic arthritis (knee pain, inflammation). This points towards a potential sexually transmitted infection (STI) that can cause disseminated infection, like *Neisseria gonorrhoeae*.\n",
      "2.  **Analyze the lab findings:** The joint fluid culture shows a bacterium that does not ferment maltose and has no polysaccharide capsule.\n",
      "    *   *Neisseria gonorrhoeae* is the most common cause of septic arthritis in young, sexually active adults.\n",
      "    *   *Neisseria gonorrhoeae* typically ferments glucose but *not* maltose.\n",
      "    *   *Neisseria gonorrhoeae* typically has a polysaccharide capsule, although some strains may lack it (non-encapsulated strains).\n",
      "    *   However, the description \"does not ferment maltose and has no polysaccharide capsule\" is slightly contradictory for *N. gonorrhoeae*. *N. gonorrhoeae* usually ferments glucose and *does* have a capsule.\n",
      "    *   Let's reconsider the possibilities. Could the description be inaccurate or refer to a different organism?\n",
      "    *   Could it be *Neisseria meningitidis*? It causes meningitis and sepsis, but septic arthritis is less common. It ferments both glucose and maltose and has a capsule.\n",
      "    *   Could it be *Neisseria meningitidis* serogroup B? This serogroup does not ferment maltose and lacks a capsule. It can cause disseminated infection, including arthritis.\n",
      "    *   Could it be *Moraxella catarrhalis*? It causes respiratory infections. It ferments both glucose and maltose and lacks a capsule. It doesn't typically cause septic arthritis.\n",
      "    *   Could it be *Haemophilus influenzae*? It causes respiratory infections and meningitis. It ferments glucose but not maltose and lacks a capsule. It doesn't typically cause septic arthritis.\n",
      "    *   Could it be *E. coli*? It causes UTIs and other infections. It ferments both glucose and maltose and has a capsule.\n",
      "    *   Could it be *Staphylococcus aureus*? A common cause of septic arthritis, but it's Gram-positive cocci, not typically described by maltose fermentation or capsule presence/absence in this context. It produces its own cell wall components.\n",
      "    *   Could it be *Streptococcus pneumoniae*? Causes pneumonia, meningitis, sepsis. It ferments both glucose and maltose and has a capsule.\n",
      "    *   Could it be *Enterococcus* species? Gram-positive cocci. They ferment some carbohydrates but not typically maltose in the way *N. gonorrhoeae* does. They lack capsules. They can cause UTIs and endocarditis, sometimes septic arthritis.\n",
      "    *   Given the clinical picture (young, sexually active, urethritis symptoms, septic arthritis), *Neisseria gonorrhoeae* is the most likely pathogen, despite the slightly conflicting lab description. The description might be simplified or slightly inaccurate. Let's assume the most likely pathogen based on the clinical context is *N. gonorrhoeae*.\n",
      "3.  **Analyze the question:** The question asks for an antibiotic that blocks cell wall synthesis.\n",
      "4.  **Evaluate the options based on mechanism of action:**\n",
      "    *   (A) Gentamicin: An aminoglycoside antibiotic. It inhibits protein synthesis by binding to the 30S ribosomal subunit.\n",
      "    *   (B) Ciprofloxacin: A fluoroquinolone antibiotic. It inhibits DNA replication by targeting DNA gyrase (topoisomerase II).\n",
      "    *   (C) Ceftriaxone: A third-generation cephalosporin antibiotic. Cephalosporins are beta-lactam antibiotics. Beta-lactams inhibit bacterial cell wall synthesis by binding to penicillin-binding proteins (PBPs), preventing the cross-linking of peptidoglycan chains.\n",
      "    *   (D) Trimethoprim: An antibiotic that inhibits folic acid synthesis by inhibiting dihydrofolate reductase.\n",
      "5.  **Connect the likely pathogen to the mechanism:**\n",
      "    *   The most likely pathogen is *Neisseria gonorrhoeae*.\n",
      "    *   The standard treatment for gonococcal infections, including disseminated gonococcal infection (DGI) causing septic arthritis, is typically a third-generation cephalosporin like ceftriaxone.\n",
      "    *   Ceftriaxone works by inhibiting cell wall synthesis.\n",
      "6.  **Conclusion:** Based on the clinical presentation strongly suggesting *Neisseria gonorrhoeae* causing septic arthritis, and knowing that ceftriaxone is a standard treatment for this condition\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this case (and many others), the 1k output limit is too low, and the model's response gets cut off before it is able to respond with the final answer."
   ],
   "metadata": {
    "id": "uemA-fYrZgld"
   }
  },
  {
   "metadata": {
    "id": "nLbKev2UlOBY"
   },
   "cell_type": "markdown",
   "source": [
    "### Evaluate tuned model performance\n",
    "\n",
    "This cell calculates the fine-tuned model's accuracy on the test data. Comparing this with the baseline score shows the improvement from fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import Gemma3Processor\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Define model and processor information (make sure the paths are right!)\n",
    "model_path = \"/content/tuned_medgemma4b/checkpoint-1700\"\n",
    "\n",
    "# Load Model and Processor\n",
    "print(\"Loading model and processor...\")\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = Gemma3Processor.from_pretrained(\"google/medgemma-1.5-4b-it\")\n",
    "\n",
    "# Run inference on the entire processed dataset\n",
    "inference_results = run_inference_batched(\n",
    "    test_dataset=test_dataset,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    batch_size=64,\n",
    ")\n",
    "results_df = pd.DataFrame(inference_results)\n",
    "results_df['model_pred'] = results_df['model_answer'].apply(extract_xml_answer)\n",
    "results_df['correct'] = results_df['ground_truth'] == results_df['model_pred']\n",
    "print('GRPO-tuned Accuracy', results_df['correct'].mean())\n",
    "results_df.to_csv('trained_results.csv') # Save trained_results"
   ],
   "metadata": {
    "id": "AuPqWJoGudfh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dfb379de-34fa-4a85-b542-3c027ce8b5ce"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GRPO-tuned Accuracy 0.7051886792452831\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reproduced MedQA **1k output** test accuracy after 1700 steps:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "| Model    | Pre-RL Tuning | Post-RL Tuning |\n",
    "| -------- | ------- | ------- |\n",
    "| medgemma-1.5-4b-it | 0.141 | 0.705 |\n",
    "\n",
    "Observations:\n",
    "- Accuracy recovery: The RL-tuning with GRPO improved the model's accuracy from a baseline of 14.1% to 70.5%.\n",
    "- Fixing the token limit issue: The baseline model often failed because it generated long reasoning chains that were cut off by the 1,000-token limit before it could state the final answer. The fine-tuned model learned to be more concise and provide the answer within the limit.\n",
    "- High efficiency: Achieving this level of performance (70.5% accuracy) on the MedQA dataset with a relatively small 4B parameter model demonstrates the effectiveness of using LoRA and GRPO for specialized domain adaptation.\n",
    "\n"
   ],
   "metadata": {
    "id": "i3lzyjX17Nex"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Additional optimizations\n",
    "Note that this notebook is meant to be a starting point. There are numerous optimizations that are not covered under this Colab, including [deepspeed](https://huggingface.co/docs/trl/main/en/deepspeed_integration), [parallelization on multiple nodes](https://huggingface.co/docs/trl/main/en/grpo_trainer#grpo-at-scale-train-a-70b-model-on-multiple-nodes), and more.\n",
    "\n",
    "We recommend checking out [GRPO Trainer](https://huggingface.co/docs/trl/main/en/grpo_trainer) for further details."
   ],
   "metadata": {
    "id": "jRcRfiEEzkOt"
   }
  },
  {
   "metadata": {
    "id": "IJV_Uw2Vfak8"
   },
   "cell_type": "markdown",
   "source": [
    "## Next steps\n",
    "\n",
    "Explore the other [notebooks](https://github.com/google-health/medgemma/blob/main/notebooks) to learn what else you can do with the model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "name": "rl_with_trl.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
