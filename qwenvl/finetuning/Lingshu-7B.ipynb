{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T13:29:43.909554Z",
     "start_time": "2025-10-29T13:29:28.982812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "import torch\n",
    "\n",
    "\n",
    "model_path=\"/root/model/Lingshu-7B\"\n",
    "# load Lingshu-7B\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    " \n",
    "# load tokenizer and processor\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    " \n",
    "# Allow gradient updates\n",
    "model.enable_input_require_grads()"
   ],
   "id": "78824b9ab29a5537",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 13:29:35.942454: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-29 13:29:36.163011: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761744576.249360   33529 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761744576.274393   33529 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761744576.476651   33529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761744576.476674   33529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761744576.476675   33529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761744576.476676   33529 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-29 13:29:36.494039: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aec030d9872c419abe19f5462b6446b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d4698ef8eb8d62ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "\n",
    "data_path = \"path/to/your/dataset.json\" \n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    train_data = data[:-4] \n",
    "    test_data = data[-4:]\n",
    "\n",
    "\n",
    "with open(\"train_data.json\", \"w\") as f:\n",
    "    json.dump(train_data, f)\n",
    "with open(\"test_data.json\", \"w\") as f:\n",
    "    json.dump(test_data, f)\n",
    "\n",
    "\n",
    "train_ds = Dataset.from_json(\"train_data.json\")"
   ],
   "id": "f21369f6c2182207"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    预处理输入数据\n",
    "    \"\"\"\n",
    "    MAX_LENGTH = 8192\n",
    "    conversation = example[\"conversations\"]\n",
    "    input_content = conversation[0][\"value\"]\n",
    "    output_content = conversation[1][\"value\"]\n",
    "\n",
    "    file_path = input_content.split(\"<|vision_start|>\")[1].split(\"<|vision_end|>\")[0]\n",
    "\n",
    "    # 构造多模态对话\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": f\"{file_path}\", \"resized_height\": 256, \"resized_width\": 256},\n",
    "                {\"type\": \"text\", \"text\": \"请描述这张图片的内容。\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    inputs = {key: value.tolist() for key, value in inputs.items()}\n",
    "    \n",
    "\n",
    "    response = tokenizer(f\"{output_content}\", add_special_tokens=False)\n",
    "    input_ids = inputs[\"input_ids\"][0] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = inputs[\"attention_mask\"][0] + response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(inputs[\"input_ids\"][0]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "\n",
    "\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "        \"labels\": torch.tensor(labels),\n",
    "        \"pixel_values\": torch.tensor(inputs[\"pixel_values\"]),\n",
    "        \"image_grid_thw\": torch.tensor(inputs[\"image_grid_thw\"]).squeeze(0)\n",
    "    }"
   ],
   "id": "3c154a61307a8bc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "train_dataset = train_ds.map(process_func)\n",
    "\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(train_dataset[0]) "
   ],
   "id": "d821f2855dd7e0f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    " \n",
    "config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False,\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    " \n",
    "\n",
    "peft_model = get_peft_model(model, config)"
   ],
   "id": "10d12a33d51bdcd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import os\n",
    " \n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output/Qwen2.5-VL-LoRA\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=74,\n",
    "    learning_rate=1e-4,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,  \n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")\n",
    " \n",
    "trainer.train()"
   ],
   "id": "e29b479ac7e01bb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from peft import PeftModel\n",
    " \n",
    "peft_model_path = \"output/Qwen2.5-VL-LoRA/checkpoint-XXX\"\n",
    "val_peft_model = PeftModel.from_pretrained(model, peft_model_path, config=config)\n",
    " \n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"path/to/image.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"请描述这张图片的内容。\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    " \n",
    "def predict(messages, model):\n",
    "    \"\"\" 用于推理验证的函数 \"\"\"\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    " \n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    # 取生成的后半部分\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    " \n",
    "response = predict(messages, val_peft_model)\n",
    "print(response)"
   ],
   "id": "18af50da9d62db09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:04:53.486593Z",
     "start_time": "2025-10-29T17:03:58.122204Z"
    }
   },
   "cell_type": "code",
   "source": "!bash ../scripts/sft_lingshu_7b_1.sh",
   "id": "ee2bd24119879085",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12874.24s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "[INFO] nproc_per_node=1 master=127.0.0.1:22162\r\n",
      "2025-10-29 17:04:07.305038: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n",
      "2025-10-29 17:04:07.312711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1761757447.321157  134247 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1761757447.323822  134247 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0000 00:00:1761757447.330994  134247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1761757447.331158  134247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1761757447.331161  134247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "W0000 00:00:1761757447.331163  134247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\r\n",
      "2025-10-29 17:04:07.333434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:37<00:00,  9.26s/it]\r\n",
      "the initlized model is /root/model/Lingshu-7B the class is Qwen2_5_VLForConditionalGeneration\r\n",
      "Vision Module - Attention Blocks:\r\n",
      "Trainable Block Indices: None\r\n",
      "Non-Trainable Block Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\r\n",
      "Merger Module Trainable: True\r\n",
      "LLM Module - Embed Tokens Trainable: True\r\n",
      "LLM Module - Trainable Layer Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\r\n",
      "LLM Module - Non-Trainable Layer Indices: None\r\n",
      "[rank0]: Traceback (most recent call last):\r\n",
      "[rank0]:   File \"/root/model/Lingshu-7B-Finetuning/qwenvl/train/train_qwen.py\", line 214, in <module>\r\n",
      "[rank0]:     train(attn_implementation=\"flash_attention_2\")\r\n",
      "[rank0]:   File \"/root/model/Lingshu-7B-Finetuning/qwenvl/train/train_qwen.py\", line 194, in train\r\n",
      "[rank0]:     data_module = make_supervised_data_module(processor, data_args=data_args)\r\n",
      "[rank0]:   File \"/root/model/Lingshu-7B-Finetuning/qwenvl/data/data_processor.py\", line 681, in make_supervised_data_module\r\n",
      "[rank0]:     train_dataset = LazySupervisedDataset(processor, data_args=data_args)\r\n",
      "[rank0]:   File \"/root/model/Lingshu-7B-Finetuning/qwenvl/data/data_processor.py\", line 251, in __init__\r\n",
      "[rank0]:     dataset_list = data_list(dataset)\r\n",
      "[rank0]:   File \"/root/model/Lingshu-7B-Finetuning/qwenvl/data/__init__.py\", line 55, in data_list\r\n",
      "[rank0]:     raise ValueError(f\"do not find {dataset_name}\")\r\n",
      "[rank0]: ValueError: do not find /root/dataset/skin/Derm1M/Derm1M_train.jsonl\r\n",
      "[rank0]:[W1029 17:04:50.668065670 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "E1029 17:04:53.206000 134214 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 134247) of binary: /root/miniforge3/envs/jupyter_env/bin/python\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/root/miniforge3/envs/jupyter_env/bin/torchrun\", line 7, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/root/miniforge3/envs/jupyter_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\r\n",
      "    return f(*args, **kwargs)\r\n",
      "  File \"/root/miniforge3/envs/jupyter_env/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in main\r\n",
      "    run(args)\r\n",
      "  File \"/root/miniforge3/envs/jupyter_env/lib/python3.10/site-packages/torch/distributed/run.py\", line 883, in run\r\n",
      "    elastic_launch(\r\n",
      "  File \"/root/miniforge3/envs/jupyter_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 139, in __call__\r\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\r\n",
      "  File \"/root/miniforge3/envs/jupyter_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 270, in launch_agent\r\n",
      "    raise ChildFailedError(\r\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n",
      "============================================================\r\n",
      "/root/model/Lingshu-7B-Finetuning/qwenvl/train/train_qwen.py FAILED\r\n",
      "------------------------------------------------------------\r\n",
      "Failures:\r\n",
      "  <NO_OTHER_FAILURES>\r\n",
      "------------------------------------------------------------\r\n",
      "Root Cause (first observed failure):\r\n",
      "[0]:\r\n",
      "  time      : 2025-10-29_17:04:53\r\n",
      "  host      : William.\r\n",
      "  rank      : 0 (local_rank: 0)\r\n",
      "  exitcode  : 1 (pid: 134247)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "============================================================\r\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
